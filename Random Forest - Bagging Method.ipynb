{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "\n",
    "Decision Trees are versatile (can do both regression and classificatiion) algorithms. \n",
    "\n",
    "Using **deep** Decision Trees we can fit complex dataset. However, it suffers from **high variance** (overfitting).\n",
    "\n",
    "One technique to reduce the high variance of Decision Trees is to train a **group of Decision Trees**, each on a different random subset of the training set. \n",
    "\n",
    "To make predictions, we just obtain the predictions of all individual trees. Then, predict the class that gets the most votes. \n",
    "\n",
    "Such an ensemble of Decision Trees is called a **Random Forest**.\n",
    "\n",
    "Despite its simplicity, this is one of the most powerful Machine Learning algorithms available today.\n",
    "\n",
    "\n",
    "## Random Forest vs. SVM & Boosting\n",
    "\n",
    "Support Vector Machines (SVMs) and boosting technique are very powerful for binary classification problems. However, these techniques do not extend naturally to **multi-class problems**.\n",
    "\n",
    "In principle, classification trees and forests work with any number of classes (~20 to ~30).\n",
    "\n",
    "In average, classification forests have shown good generalization, even in problems with **high dimensionality**. \n",
    "\n",
    "\n",
    "## Training Random Forests: Bagging\n",
    "\n",
    "A random decision forest is generally trained via the **bagging** method (or sometimes pasting).\n",
    "\n",
    "Bagging stands for **bootstrap aggregation**.\n",
    "\n",
    "In bootstrapping we use the same training algorithm for every predictor, but to **train them on different random subsets** of the training set. \n",
    "- When sampling is performed with replacement, this method is called bagging. \n",
    "- When sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "\n",
    "<img src=\"https://cse.unl.edu/~hasan/Pics/Bagging.png\" width=700 height=400>\n",
    "\n",
    "\n",
    "## How Do Random Forests Reduce Variance to Improve Generalizability?\n",
    "\n",
    "A key aspect of random forests is the fact that its component trees are all **randomly different from one another**. \n",
    "\n",
    "This leads to **de-correlation between the individual tree predictions** and, in turn, to improved generalization.\n",
    "\n",
    "Forest randomness also helps achieve high robustness with respect to **noisy data**.\n",
    "\n",
    "\n",
    "## Randomization Techniques for Training Random Forests\n",
    "\n",
    "Randomness is injected into the trees during the training phase. \n",
    "\n",
    "Two of the most popular ways of doing so are:\n",
    "     - Random training data set sampling  (e.g., bagging) \n",
    "     - Randomized node optimization (RNO)\n",
    "\n",
    "\n",
    "Unlike bagging, RNO uses use all available training data and controls the randomness by varying the number of parameters for training.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "In this notebook, we perform the following tasks.\n",
    "\n",
    "1. **Explore the bagging method** for training Random Forests. \n",
    "2. Investigate whether dimensionality reduction technique (e.g., PCA) improves the performance of Decision Tree/Random Forest.\n",
    "3. Use Random Forest for feature selection.\n",
    "4. Study a variant of Random Forests that is known as extremely randomized trees or **Extra-Trees**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST\n",
    "\n",
    "\n",
    "We will use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data using Scikit-Learn\n",
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "X = mnist[\"data\"].astype('float64')\n",
    "y = mnist[\"target\"].astype('int64')\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Explore the Bagging Method for Growing (Training) Random Forests\n",
    "\n",
    "\n",
    "\n",
    "To reduce the variance of a Decision Tree we grow a Random Forest.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees.\n",
    "\n",
    "Instead of searching for the very best feature when splitting a node, it searches for the best feature among a **random subset of features**. This results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "\n",
    "A Random Forest is an **ensemble of Decision Trees**. It is generally trained via the bagging method (or sometimes pasting). \n",
    "\n",
    "Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, we can instead use the **RandomForestClassifier class**, which is more convenient and optimized for Decision Trees (similarly, there is a RandomForestRegressor class for regression tasks). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: Key Hyperparameters\n",
    "\n",
    "\n",
    "We will use the following hyperparameters of the RandomForestClassifier class. \n",
    "\n",
    "For a full list of the hyperparameters visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "\n",
    "- n_estimators : The number of trees in the forest. Default=10\n",
    "\n",
    "\n",
    "- criterion : The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. Default=”gini”\n",
    "\n",
    "\n",
    "- max_depth : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default=None\n",
    "\n",
    "\n",
    "- min_samples_split : The minimum number of samples required to split an internal node: Default=2\n",
    "\n",
    "        -- If int, then consider min_samples_split as the minimum number.\n",
    "        -- If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "        \n",
    "        \n",
    "        \n",
    "- min_samples_leaf : The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. Default=1\n",
    "\n",
    "\n",
    "         -- If int, then consider min_samples_leaf as the minimum number.\n",
    "         -- If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "- max_features : The number of features to consider when looking for the best split. Default=\"auto\".\n",
    "\n",
    "        -- If int, then consider max_features features at each split.\n",
    "        -- If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "        -- If “auto”, then max_features=sqrt(n_features).\n",
    "        -- If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        -- If “log2”, then max_features=log2(n_features).\n",
    "        -- If None, then max_features=n_features.\n",
    "\n",
    "\n",
    "- max_leaf_nodes : Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. Default=None\n",
    "\n",
    "\n",
    "- class_weight : Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Default=None\n",
    "\n",
    "         -- The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "\n",
    "\n",
    "- oob_score : Whether to use out-of-bag samples to estimate the generalization accuracy. Default=False\n",
    "\n",
    "        -- When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. Then, we can get the score of the training dataset obtained using an out-of-bag estimate (use the $oob\\_score\\_$ attribute)\n",
    "        \n",
    "        \n",
    "- bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. Default=True\n",
    " \n",
    "\n",
    "- verbose : Controls the verbosity when fitting and predicting. Default=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training took 142.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9694285714285714\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1369    0    1    0    2    2    6    0    7    0]\n",
      " [   0 1560    5    5    3    0    2    2    2    1]\n",
      " [   2    2 1406    7    4    1    5    8    7    1]\n",
      " [   1    1   21 1364    0   14    0   11   20    3]\n",
      " [   1    0    1    0 1315    0    6    4    2   21]\n",
      " [   1    2    3   10    2 1187   12    1    9    4]\n",
      " [   7    2    1    0    3    7 1362    0    5    0]\n",
      " [   3    6   18    0   10    0    0 1400    4   17]\n",
      " [   0    9    5    5    5    4    3    0 1318   19]\n",
      " [   4    1    4   21   11    4    2   15    8 1291]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1387\n",
      "           1       0.99      0.99      0.99      1580\n",
      "           2       0.96      0.97      0.97      1443\n",
      "           3       0.97      0.95      0.96      1435\n",
      "           4       0.97      0.97      0.97      1350\n",
      "           5       0.97      0.96      0.97      1231\n",
      "           6       0.97      0.98      0.98      1387\n",
      "           7       0.97      0.96      0.97      1458\n",
      "           8       0.95      0.96      0.96      1368\n",
      "           9       0.95      0.95      0.95      1361\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate:  0.97125\n",
      "\n",
      "\n",
      "CPU times: user 12min 56s, sys: 15.3 s, total: 13min 11s\n",
      "Wall time: 2min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "training_forest_clf = t1 - t0\n",
    "\n",
    "print(\"Random Forest Training took {:.2f}s\".format(training_forest_clf))\n",
    "\n",
    "y_test_predicted = forest_clf.predict(X_test)\n",
    "accuracy_forest_clf = accuracy_score(y_test, y_test_predicted)\n",
    "print(\"\\nTest Accuracy: \", accuracy_forest_clf)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", forest_clf.oob_score_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Investigate Whether Dimensionality Reduction Technique (e.g., PCA) Improves the Performance of Decision Tree/Random Forest\n",
    "\n",
    "We know that it is useful to **find disciminative features** for growing a Decision Tree. For this we have performing feature selection.\n",
    "\n",
    "Now we would like to see whether we can extract features to grow a Decision Tree.\n",
    "\n",
    "We use dimensionality reduction to extract features.\n",
    "\n",
    "We apply the Principle Component Analysis (PCA) dimensionality reduction technique to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance (95%).\n",
    "\n",
    "The PCA **extracts 154 features** which we use to train a Random Forest classifier.\n",
    "\n",
    "Our goal is to see whether the extracted features improve the performance of the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principle Components (Extracted Features):  154\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Number of Principle Components (Extracted Features): \", pca.n_components_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (PCA):  0.9497142857142857\n",
      "\n",
      "Test Confusion Matrix (PCA):\n",
      "[[1358    0    4    2    4    3   10    2    4    0]\n",
      " [   0 1553   10    5    2    0    4    2    4    0]\n",
      " [   6    3 1361   14   13    2    4    7   30    3]\n",
      " [   4    0   21 1326    1   23    4   17   30    9]\n",
      " [   1    1    6    1 1298    0    8    2    6   27]\n",
      " [   4    2    6   26    9 1157   12    3    4    8]\n",
      " [   9    1    5    1    7   14 1348    0    2    0]\n",
      " [   2    5   21    1   15    1    0 1384   10   19]\n",
      " [   1   10   12   31    8   23    4    1 1261   17]\n",
      " [   7    1    5   23   35    8    2   24    6 1250]]\n",
      "\n",
      "Classification Report (PCA):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1387\n",
      "           1       0.99      0.98      0.98      1580\n",
      "           2       0.94      0.94      0.94      1443\n",
      "           3       0.93      0.92      0.93      1435\n",
      "           4       0.93      0.96      0.95      1350\n",
      "           5       0.94      0.94      0.94      1231\n",
      "           6       0.97      0.97      0.97      1387\n",
      "           7       0.96      0.95      0.95      1458\n",
      "           8       0.93      0.92      0.93      1368\n",
      "           9       0.94      0.92      0.93      1361\n",
      "\n",
      "    accuracy                           0.95     14000\n",
      "   macro avg       0.95      0.95      0.95     14000\n",
      "weighted avg       0.95      0.95      0.95     14000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate (PCA):  0.97125\n",
      "\n",
      "\n",
      "CPU times: user 25min 40s, sys: 11 s, total: 25min 51s\n",
      "Wall time: 3min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf_pca = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_predicted = forest_clf_pca.predict(X_test_pca)\n",
    "print(\"Test Accuracy (PCA): \", accuracy_score(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix (PCA):\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report (PCA):\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate (PCA): \", forest_clf.oob_score_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: Dimensionality Reduction with Random Forest\n",
    "\n",
    "We observe that PCA **lowers the performance** of a Random Forest classifier. This is due to the fact that extracted features (original features projected into a lower dimension) don't meaningfully combine to produce effective desion rules.\n",
    "\n",
    "Machine Learning algorithms that create decision rules by composing knowledge (e.g., decision tree uses a combination of the features to contruct a decision rule) don't benefit from dimensionality reduction. \n",
    "\n",
    "Thus, **we shouldn't use PCA with Decision Tree or Random Forest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Use Random Forest for Feature Selection\n",
    "\n",
    "Random Forest can be used for feature selection by using **feature importance**.\n",
    "\n",
    "**Feature Selection** is a very useful Machine Learning technique that we often use. Random Forests are very handy to get a quick understanding of what features actually matter. This helps to perform feature selection.\n",
    "\n",
    "Random Forests enable us to perform feature selection by finding the **relative importance of each feature**. \n",
    "\n",
    "A Random Forest measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). \n",
    "\n",
    "More precisely, it is a **weighted average**, where each node’s weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. \n",
    "\n",
    "We can access the result using the $feature\\_importances\\_$ variable. \n",
    "\n",
    "Following example shows the plot of each pixel's importance in the MNIST dataset after trained by a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pixel importances on 28*28 image\n",
    "importances = forest_clf.feature_importances_\n",
    "image = importances.reshape(28, 28)\n",
    "\n",
    "# Plot pixel importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"MNIST Pixel Importance\")\n",
    "\n",
    "plt.imshow(image, cmap=mpl.cm.hot,interpolation=\"nearest\")\n",
    "cbar = plt.colorbar(ticks=[forest_clf.feature_importances_.min(), forest_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Study a Variant of Random Forest - Extremely Randomized Trees or Extra-Trees\n",
    "\n",
    "Random Forests are used to reduce the variance of Decision Trees. \n",
    "\n",
    "Interestingly, we can **reduce the variance** of the Random Forests by growing **extremely randomized trees**. In Scikit-Learn it is implemented by the ExtraTreesClassifier and ExtraTreesRegressor classes.\n",
    "\n",
    "These Extra-Trees take randomness one step further by using **random thresholds** for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).\n",
    "\n",
    "Thus, Extra-Trees trade **more bias for a lower variance**.\n",
    "\n",
    "Extra-Trees are much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.\n",
    "\n",
    "Scikit-Learn’s ExtraTreesClassifier API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class.\n",
    "\n",
    "### Note:\n",
    "By deault the ExtraTreesClassifier \"bootstrap\" hyperparameter is set to False. As a consequence, the whole dataset is used to build each tree. To using the bagging method, we need to explicity set it to True.\n",
    "\n",
    "\n",
    "### RandomForestClassifier vs. ExtraTreesClassifier\n",
    "\n",
    "It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. \n",
    "\n",
    "Generally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Training took 103.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9674285714285714\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1367    0    1    0    2    3    7    0    7    0]\n",
      " [   0 1562    6    4    3    0    1    1    2    1]\n",
      " [   3    3 1399    7    3    1    5    9   12    1]\n",
      " [   2    1   19 1359    0   17    2   11   18    6]\n",
      " [   2    0    0    0 1313    0    5    4    1   25]\n",
      " [   1    2    4   10    2 1187   10    1    9    5]\n",
      " [   9    2    1    0    4    7 1361    0    3    0]\n",
      " [   4    7   21    0    6    0    0 1390    4   26]\n",
      " [   0    9    5    8    5    6    2    0 1314   19]\n",
      " [   4    2    4   19   11    6    2   14    7 1292]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1387\n",
      "           1       0.98      0.99      0.99      1580\n",
      "           2       0.96      0.97      0.96      1443\n",
      "           3       0.97      0.95      0.96      1435\n",
      "           4       0.97      0.97      0.97      1350\n",
      "           5       0.97      0.96      0.97      1231\n",
      "           6       0.98      0.98      0.98      1387\n",
      "           7       0.97      0.95      0.96      1458\n",
      "           8       0.95      0.96      0.96      1368\n",
      "           9       0.94      0.95      0.94      1361\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate:  0.9700714285714286\n",
      "\n",
      "\n",
      "CPU times: user 8min 28s, sys: 15.6 s, total: 8min 43s\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                       max_depth=32, class_weight=\"balanced\", oob_score=True, \n",
    "                                       bootstrap=True, verbose=1, n_jobs=-1)\n",
    "extra_trees_clf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "training_extra_tree = t1 - t0\n",
    "\n",
    "print(\"Extra Tree Training took {:.2f}s\".format(training_extra_tree))\n",
    "\n",
    "\n",
    "y_test_predicted = extra_trees_clf.predict(X_test)\n",
    "accuracy_extra_trees = accuracy_score(y_test, y_test_predicted)\n",
    "print(\"\\nTest Accuracy: \", accuracy_extra_trees)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", extra_trees_clf.oob_score_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (1000 trees)</td>\n",
       "      <td>0.969429</td>\n",
       "      <td>142.620684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra-Trees (1000 trees)</td>\n",
       "      <td>0.967429</td>\n",
       "      <td>103.340217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  Accuracy  Running-Time\n",
       "0  Random Forest (1000 trees)  0.969429    142.620684\n",
       "1    Extra-Trees (1000 trees)  0.967429    103.340217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"Random Forest (1000 trees)\", accuracy_forest_clf, training_forest_clf], \n",
    "        [\"Extra-Trees (1000 trees)\", accuracy_extra_trees, training_extra_tree]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Classifier\", \"Accuracy\", \"Running-Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Understanding\n",
    "\n",
    "Based on the performance results we see that Extra-Trees and Random Forests are comparable. \n",
    "\n",
    "- Extra-Trees are **faster at the cost of slightly higher bias** (i.e., slightly smaller test accuracy). The speed in training comes from using random threshold then searching for an optimal threshold.\n",
    "- The performance difference is insignificant.\n",
    "\n",
    "To get a better comparative understanding we should tune the hyperparameters for these two models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
